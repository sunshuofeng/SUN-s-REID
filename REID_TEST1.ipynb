{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "REID_TEST1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uVSkPA9dREsf",
        "3HQcNH5LMilp",
        "K-6Dc1U3M_5_",
        "-LbctDphPFgB",
        "CtwWsO03jp5P",
        "6GcAv-6SlFfh"
      ],
      "mount_file_id": "1T7HNwgtNGdRapM9NQ4oxjwruQ5R7Mrbp",
      "authorship_tag": "ABX9TyMy///mBRXq74jAa262t0TC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshuofeng/SUN-s-REID/blob/master/REID_TEST1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSBQLax1AC88"
      },
      "source": [
        "#加载kaggle文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwJGxCWF9q_-",
        "outputId": "41143546-58b2-4175-da1e-3236a7ca87a9"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.10)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOreY7Kq-nFy"
      },
      "source": [
        "import json\r\n",
        "token = {\"username\":\"ssfailearning\",\"key\":\"bccedfffce653f07d5acaf4409341c47\"}\r\n",
        "with open('/content/kaggle.json', 'w') as file:\r\n",
        "  json.dump(token, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSUCpQuo_sAK"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG0vAfu6_-rY",
        "outputId": "9e5b31dd-90ec-4925-9bd2-3231b5de8728"
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json\r\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VPVygJ6AO9l",
        "outputId": "212369ea-e7fe-4f45-8fd2-944088a0521c"
      },
      "source": [
        "!kaggle datasets download -d rayiooo/reid_market-1501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reid_market-1501.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-777-IiMBm-I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pHZOn1hNueU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Fr2DOSN6Xg"
      },
      "source": [
        "# <font color=red>***配置文件***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIT9OW4ZN8FD"
      },
      "source": [
        "cfg={\r\n",
        "    'train_size':[384,128],\r\n",
        "    'test_size':[384,128],\r\n",
        "    'padding':10,\r\n",
        "    'mean':[0.485, 0.456, 0.406],\r\n",
        "    'std':[0.229, 0.224, 0.225],\r\n",
        "\r\n",
        "    'Multi_Data':False,\r\n",
        "    'num_workers':8,\r\n",
        "    'SAMPLER':'',\r\n",
        "\r\n",
        "    'train_bs':48,\r\n",
        "    'test_bs':48,\r\n",
        "\r\n",
        "    'train_K_instances':12,\r\n",
        "     \r\n",
        "    'epochs':70,\r\n",
        "    'ckpt':'reid_test1.pt',\r\n",
        "    'logpt':'test1.log',\r\n",
        "\r\n",
        "    'rerank':False,\r\n",
        "   \r\n",
        "    'lr':3e-4,\r\n",
        "    'momentum':0.9,\r\n",
        "    'weight_decay':5e-4,\r\n",
        "     \r\n",
        "    'margin':0.3,\r\n",
        "     \r\n",
        "    'steps':[35,55],\r\n",
        "    'warmup_iters':10,\r\n",
        "    'warmup_factors':1/3,\r\n",
        "    'gamma':0.1,\r\n",
        "     \r\n",
        "     \r\n",
        "\r\n",
        "    'amp':True,\r\n",
        "    'ema':False,\r\n",
        "    'swa':False\r\n",
        "    \r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9xfXfqJNvNH"
      },
      "source": [
        "# <font color=red>***读取数据***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVSkPA9dREsf"
      },
      "source": [
        "##<font color=blue>***取market1501***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkosuYLnNxBo"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "# os.makedirs('datasets')\r\n",
        "zip_file = zipfile.ZipFile('/content/datasets/rayiooo/reid_market-1501/reid_market-1501.zip')\r\n",
        "for names in zip_file.namelist():\r\n",
        "        zip_file.extract(names,'datasets')\r\n",
        "zip_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGwfmPqhPBWp"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from torch.cuda.amp import GradScaler,autocast\r\n",
        "import torch\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiEL5941NDEX"
      },
      "source": [
        "##<font color=blue>***数据增强***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOSE54gbzWn"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HQcNH5LMilp"
      },
      "source": [
        "#### <font color=yellow>***随机擦除***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40V4tXtuMiSQ"
      },
      "source": [
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "\r\n",
        "class RandomErasing(object):\r\n",
        "    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\r\n",
        "        'Random Erasing Data Augmentation' by Zhong et al.\r\n",
        "        See https://arxiv.org/pdf/1708.04896.pdf\r\n",
        "    Args:\r\n",
        "         probability: The probability that the Random Erasing operation will be performed.\r\n",
        "         sl: Minimum proportion of erased area against input image.\r\n",
        "         sh: Maximum proportion of erased area against input image.\r\n",
        "         r1: Minimum aspect ratio of erased area.\r\n",
        "         mean: Erasing value.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=(0.4914, 0.4822, 0.4465)):\r\n",
        "        self.probability = probability\r\n",
        "        self.mean = mean\r\n",
        "        self.sl = sl\r\n",
        "        self.sh = sh\r\n",
        "        self.r1 = r1\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "\r\n",
        "        if random.uniform(0, 1) >= self.probability:\r\n",
        "            return img\r\n",
        "\r\n",
        "        for attempt in range(100):\r\n",
        "            area = img.size()[1] * img.size()[2]\r\n",
        "\r\n",
        "            target_area = random.uniform(self.sl, self.sh) * area\r\n",
        "            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\r\n",
        "\r\n",
        "            h = int(round(math.sqrt(target_area * aspect_ratio)))\r\n",
        "            w = int(round(math.sqrt(target_area / aspect_ratio)))\r\n",
        "\r\n",
        "            if w < img.size()[2] and h < img.size()[1]:\r\n",
        "                x1 = random.randint(0, img.size()[1] - h)\r\n",
        "                y1 = random.randint(0, img.size()[2] - w)\r\n",
        "                if img.size()[0] == 3:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                    img[1, x1:x1 + h, y1:y1 + w] = self.mean[1]\r\n",
        "                    img[2, x1:x1 + h, y1:y1 + w] = self.mean[2]\r\n",
        "                else:\r\n",
        "                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\r\n",
        "                return img\r\n",
        "\r\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-6Dc1U3M_5_"
      },
      "source": [
        "####<font color=yellow>***其余数据增强（torchvision transformss）***</font>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFo4pix1VcKT"
      },
      "source": [
        "\r\n",
        "import torchvision.transforms as T\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_transforms(cfg, training=True):\r\n",
        "    normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\r\n",
        "    if training:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['train_size']),\r\n",
        "            T.RandomHorizontalFlip(p=0.5),\r\n",
        "            T.Pad(cfg['padding']),\r\n",
        "            T.RandomCrop(cfg['train_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform,\r\n",
        "            RandomErasing(probability=0.5, mean=cfg['mean'])\r\n",
        "        ])\r\n",
        "    else:\r\n",
        "        transform = T.Compose([\r\n",
        "            T.Resize(cfg['test_size']),\r\n",
        "            T.ToTensor(),\r\n",
        "            normalize_transform\r\n",
        "        ])\r\n",
        "\r\n",
        "    return transform\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LbctDphPFgB"
      },
      "source": [
        "##<font color=blue>***sampler***</font>\r\n",
        "\r\n",
        "<font color=red>抽取N个pid(即N个人)，每个人抽取K张图片，目的是为了能确保至少有三元组，**锚，正样本，负样本**</font>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoJVgt7sb4TU"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9S_5uSFN46Z"
      },
      "source": [
        "\r\n",
        "import copy\r\n",
        "import random\r\n",
        "import torch\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "\r\n",
        "class RandomIdentitySampler(Sampler):\r\n",
        "    \"\"\"\r\n",
        "    Randomly sample N identities, then for each identity,\r\n",
        "    randomly sample K instances, therefore batch size is N*K.\r\n",
        "    Args:\r\n",
        "    - data_source (list): list of (img_path, pid, camid).\r\n",
        "    - num_instances (int): number of instances per identity in a batch.\r\n",
        "    - batch_size (int): number of examples in a batch.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_source, batch_size, num_instances):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_instances = num_instances\r\n",
        "        self.num_pids_per_batch = self.batch_size // self.num_instances\r\n",
        "        self.index_dic = defaultdict(list)\r\n",
        "        for index, (_, pid, _) in enumerate(self.data_source):\r\n",
        "            self.index_dic[pid].append(index)\r\n",
        "        self.pids = list(self.index_dic.keys())\r\n",
        "\r\n",
        "        # estimate number of examples in an epoch\r\n",
        "        self.length = 0\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = self.index_dic[pid]\r\n",
        "            num = len(idxs)\r\n",
        "            if num < self.num_instances:\r\n",
        "                num = self.num_instances\r\n",
        "            self.length += num - num % self.num_instances\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        batch_idxs_dict = defaultdict(list)\r\n",
        "\r\n",
        "        for pid in self.pids:\r\n",
        "            idxs = copy.deepcopy(self.index_dic[pid])\r\n",
        "            if len(idxs) < self.num_instances:\r\n",
        "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\r\n",
        "            random.shuffle(idxs)\r\n",
        "            batch_idxs = []\r\n",
        "            for idx in idxs:\r\n",
        "                batch_idxs.append(idx)\r\n",
        "                if len(batch_idxs) == self.num_instances:\r\n",
        "                    batch_idxs_dict[pid].append(batch_idxs)\r\n",
        "                    batch_idxs = []\r\n",
        "\r\n",
        "        avai_pids = copy.deepcopy(self.pids)\r\n",
        "        final_idxs = []\r\n",
        "\r\n",
        "        while len(avai_pids) >= self.num_pids_per_batch:\r\n",
        "            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)\r\n",
        "            for pid in selected_pids:\r\n",
        "                batch_idxs = batch_idxs_dict[pid].pop(0)\r\n",
        "                final_idxs.extend(batch_idxs)\r\n",
        "                if len(batch_idxs_dict[pid]) == 0:\r\n",
        "                    avai_pids.remove(pid)\r\n",
        "\r\n",
        "        self.length = len(final_idxs)\r\n",
        "        return iter(final_idxs)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.length\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzMoTzRBS8WK"
      },
      "source": [
        "##<font color=blue>***建立数据集***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7OPh6Fabn__"
      },
      "source": [
        "###<font color=green>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBqvtdO-TA6N"
      },
      "source": [
        "####<font color=yellow>***图像读取数据集（market1501）***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a4BoMPfTDpS"
      },
      "source": [
        "import glob\r\n",
        "import re\r\n",
        "import os.path as osp\r\n",
        "class Market1501():\r\n",
        "    def __init__(self, root='/content/datasets', verbose=True, **kwargs):\r\n",
        "        super(Market1501, self).__init__()\r\n",
        "        self.dataset_dir = root\r\n",
        "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\r\n",
        "        self.query_dir = osp.join(self.dataset_dir, 'query')\r\n",
        "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\r\n",
        "        self._check_before_run()\r\n",
        "\r\n",
        "        train = self._process_dir(self.train_dir, relabel=True)\r\n",
        "        query = self._process_dir(self.query_dir, relabel=False)\r\n",
        "        gallery = self._process_dir(self.gallery_dir, relabel=False)\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print(\"=> Market1501 loaded\")\r\n",
        "            self.print_dataset_statistics(train, query, gallery)\r\n",
        "\r\n",
        "        self.train = train\r\n",
        "        self.query = query\r\n",
        "        self.gallery = gallery\r\n",
        "\r\n",
        "        self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\r\n",
        "        self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\r\n",
        "        self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\r\n",
        "\r\n",
        "    def print_dataset_statistics(self, train, query, gallery):\r\n",
        "        num_train_pids, num_train_imgs, num_train_cams = self.get_imagedata_info(train)\r\n",
        "        num_query_pids, num_query_imgs, num_query_cams = self.get_imagedata_info(query)\r\n",
        "        num_gallery_pids, num_gallery_imgs, num_gallery_cams = self.get_imagedata_info(gallery)\r\n",
        "\r\n",
        "        print(\"Dataset statistics:\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  subset   | # ids | # images | # cameras\")\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "        print(\"  train    | {:5d} | {:8d} | {:9d}\".format(num_train_pids, num_train_imgs, num_train_cams))\r\n",
        "        print(\"  query    | {:5d} | {:8d} | {:9d}\".format(num_query_pids, num_query_imgs, num_query_cams))\r\n",
        "        print(\"  gallery  | {:5d} | {:8d} | {:9d}\".format(num_gallery_pids, num_gallery_imgs, num_gallery_cams))\r\n",
        "        print(\"  ----------------------------------------\")\r\n",
        "    \r\n",
        "    def get_imagedata_info(self, data):\r\n",
        "        pids, cams = [], []\r\n",
        "        for _, pid, camid in data:\r\n",
        "            pids += [pid]\r\n",
        "            cams += [camid]\r\n",
        "        pids = set(pids)\r\n",
        "        cams = set(cams)\r\n",
        "        num_pids = len(pids)\r\n",
        "        num_cams = len(cams)\r\n",
        "        num_imgs = len(data)\r\n",
        "        return num_pids, num_imgs, num_cams\r\n",
        "    \r\n",
        "\r\n",
        "    def _check_before_run(self):\r\n",
        "        \"\"\"Check if all files are available before going deeper\"\"\"\r\n",
        "        if not osp.exists(self.dataset_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\r\n",
        "        if not osp.exists(self.train_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\r\n",
        "        if not osp.exists(self.query_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\r\n",
        "        if not osp.exists(self.gallery_dir):\r\n",
        "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\r\n",
        "\r\n",
        "    \r\n",
        "    ##最终返回的是每张图片的路径，pid，camid\r\n",
        "    def _process_dir(self, dir_path, relabel=False):\r\n",
        "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\r\n",
        "\r\n",
        "        ##匹配文件名中的有用信息，0001_c1代表pid=1，cid=1，\r\n",
        "        ##至于为什么是[-\\d]+,是因为有-1_c1这种的存在，也应该匹配上\r\n",
        "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\r\n",
        "\r\n",
        "        pid_container = set()\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, _ = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            pid_container.add(pid)\r\n",
        "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\r\n",
        "\r\n",
        "        dataset = []\r\n",
        "        for img_path in img_paths:\r\n",
        "            pid, camid = map(int, pattern.search(img_path).groups())\r\n",
        "            if pid == -1: continue  # junk images are just ignored\r\n",
        "            assert 0 <= pid <= 1501  # pid == 0 means background\r\n",
        "            assert 1 <= camid <= 6\r\n",
        "            camid -= 1  # index starts from 0\r\n",
        "            if relabel: pid = pid2label[pid]\r\n",
        "            dataset.append((img_path, pid, camid))\r\n",
        "\r\n",
        "        return dataset\r\n",
        "\r\n",
        "\r\n",
        "import torch.utils.data as Data\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "\r\n",
        "class ImageDataset(Data.Dataset):\r\n",
        "    def __init__(self, dataset, transform=None):\r\n",
        "        self.dataset = dataset\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.dataset)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        img_path, pid, camid = self.dataset[index]\r\n",
        "        img = Image.open(img_path)\r\n",
        "        if self.transform is not None:\r\n",
        "            img = self.transform(img)\r\n",
        "        return img, pid, camid, img_path\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUNQLWzGbCme"
      },
      "source": [
        "###<font color=green>***建立dataloader***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63cQkqjFbGL1"
      },
      "source": [
        "\r\n",
        "###对于训练的时候我们不需要关心摄像头的问题，我们只需要让模型能认出图片里的人就行\r\n",
        "def train_collate_fn(batch):\r\n",
        "    imgs, pids, _, _, = zip(*batch)\r\n",
        "    pids = torch.tensor(pids, dtype=torch.int64)\r\n",
        "    return torch.stack(imgs, dim=0), pids\r\n",
        "\r\n",
        "###对于验证集而言，为了提高验证的真实性，我们应该防止同一摄像头的图片进入验证（同一摄像头相当于数据泄露）\r\n",
        "def val_collate_fn(batch):\r\n",
        "    imgs, pids, camids, _ = zip(*batch)\r\n",
        "    return torch.stack(imgs, dim=0), pids, camids\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def make_data_loader(datasets,cfg):\r\n",
        "    train_transform=build_transforms(cfg,training=True)\r\n",
        "    test_transform=build_transforms(cfg,training=False)\r\n",
        "    if cfg['Multi_Data']:\r\n",
        "        data=datasets[0]\r\n",
        "        for dataset in datasets[1:]:\r\n",
        "            data.train.extend(dataset.train)\r\n",
        "            data.query.extend(dataset.query)\r\n",
        "            data.gallery.extend(dataset.gallery)\r\n",
        "\r\n",
        "            data.num_train_pids+=dataset.num_train_pids\r\n",
        "    else:\r\n",
        "        data=datasets\r\n",
        "\r\n",
        "    num_classes=data.num_train_pids\r\n",
        "    train_set=ImageDataset(data.train,train_transform)\r\n",
        "\r\n",
        "    if cfg['SAMPLER']=='softmax':\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     shuffle=True,collate_fn=train_collate_fn)\r\n",
        "    else:\r\n",
        "        train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\r\n",
        "                                     sampler=RandomIdentitySampler(data.train,\r\n",
        "                                    cfg['train_bs'],cfg['train_K_instances']),\r\n",
        "                                    num_workers=cfg['num_workers'],\r\n",
        "                                    collate_fn=train_collate_fn)\r\n",
        "    \r\n",
        "    val_set = ImageDataset(data.query + data.gallery, test_transform)\r\n",
        "    val_loader = Data.DataLoader(\r\n",
        "        val_set, batch_size=cfg['test_bs'], shuffle=False, num_workers=cfg['num_workers'],\r\n",
        "        collate_fn=val_collate_fn\r\n",
        "    )\r\n",
        "    return train_loader, val_loader, len(data.query), num_classes\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrTHGA10c2Ug"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW_0tNfkO4JZ"
      },
      "source": [
        "#<font color=red>***模型建立***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTBDCyLMehBG"
      },
      "source": [
        "##<font color=blue>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGKKFCmvejZ_"
      },
      "source": [
        "###<font color=green>***strong-baseline***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZZXgt-O5jV"
      },
      "source": [
        "from torchvision.models import resnet50\r\n",
        "class Baseline(nn.Module):\r\n",
        "    in_planes = 2048\r\n",
        "    def __init__(self, num_classes, neck='bnneck', neck_feat='after'):\r\n",
        "        super(Baseline, self).__init__()\r\n",
        "#         if model_name == 'resnet18':\r\n",
        "#             self.in_planes = 512\r\n",
        "#             self.base = ResNet(last_stride=last_stride, \r\n",
        "#                                block=BasicBlock, \r\n",
        "#                                layers=[2, 2, 2, 2])\r\n",
        "#         elif model_name == 'resnet34':\r\n",
        "#             self.in_planes = 512\r\n",
        "#             self.base = ResNet(last_stride=last_stride,\r\n",
        "#                                block=BasicBlock,\r\n",
        "#                                layers=[3, 4, 6, 3])\r\n",
        "#         elif model_name == 'resnet50':\r\n",
        "#             self.base = ResNet(last_stride=last_stride,\r\n",
        "#                                block=Bottleneck,\r\n",
        "#                                layers=[3, 4, 6, 3])\r\n",
        "#         elif model_name == 'resnet101':\r\n",
        "#             self.base = ResNet(last_stride=last_stride,\r\n",
        "#                                block=Bottleneck, \r\n",
        "#                                layers=[3, 4, 23, 3])\r\n",
        "#         elif model_name == 'resnet152':\r\n",
        "#             self.base = ResNet(last_stride=last_stride, \r\n",
        "#                                block=Bottleneck,\r\n",
        "#                                layers=[3, 8, 36, 3])\r\n",
        "            \r\n",
        "#         elif model_name == 'se_resnet50':\r\n",
        "#             self.base = SENet(block=SEResNetBottleneck, \r\n",
        "#                               layers=[3, 4, 6, 3], \r\n",
        "#                               groups=1, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=None, \r\n",
        "#                               inplanes=64, \r\n",
        "#                               input_3x3=False,\r\n",
        "#                               downsample_kernel_size=1, \r\n",
        "#                               downsample_padding=0,\r\n",
        "#                               last_stride=last_stride) \r\n",
        "#         elif model_name == 'se_resnet101':\r\n",
        "#             self.base = SENet(block=SEResNetBottleneck, \r\n",
        "#                               layers=[3, 4, 23, 3], \r\n",
        "#                               groups=1, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=None, \r\n",
        "#                               inplanes=64, \r\n",
        "#                               input_3x3=False,\r\n",
        "#                               downsample_kernel_size=1, \r\n",
        "#                               downsample_padding=0,\r\n",
        "#                               last_stride=last_stride)\r\n",
        "#         elif model_name == 'se_resnet152':\r\n",
        "#             self.base = SENet(block=SEResNetBottleneck, \r\n",
        "#                               layers=[3, 8, 36, 3],\r\n",
        "#                               groups=1, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=None, \r\n",
        "#                               inplanes=64, \r\n",
        "#                               input_3x3=False,\r\n",
        "#                               downsample_kernel_size=1, \r\n",
        "#                               downsample_padding=0,\r\n",
        "#                               last_stride=last_stride)  \r\n",
        "#         elif model_name == 'se_resnext50':\r\n",
        "#             self.base = SENet(block=SEResNeXtBottleneck,\r\n",
        "#                               layers=[3, 4, 6, 3], \r\n",
        "#                               groups=32, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=None, \r\n",
        "#                               inplanes=64, \r\n",
        "#                               input_3x3=False,\r\n",
        "#                               downsample_kernel_size=1, \r\n",
        "#                               downsample_padding=0,\r\n",
        "#                               last_stride=last_stride) \r\n",
        "#         elif model_name == 'se_resnext101':\r\n",
        "#             self.base = SENet(block=SEResNeXtBottleneck,\r\n",
        "#                               layers=[3, 4, 23, 3], \r\n",
        "#                               groups=32, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=None, \r\n",
        "#                               inplanes=64, \r\n",
        "#                               input_3x3=False,\r\n",
        "#                               downsample_kernel_size=1, \r\n",
        "#                               downsample_padding=0,\r\n",
        "#                               last_stride=last_stride)\r\n",
        "#         elif model_name == 'senet154':\r\n",
        "#             self.base = SENet(block=SEBottleneck, \r\n",
        "#                               layers=[3, 8, 36, 3],\r\n",
        "#                               groups=64, \r\n",
        "#                               reduction=16,\r\n",
        "#                               dropout_p=0.2, \r\n",
        "#                               last_stride=last_stride)\r\n",
        "#         elif model_name == 'resnet50_ibn_a':\r\n",
        "#             self.base = resnet50_ibn_a(last_stride)\r\n",
        "\r\n",
        "#         if pretrain_choice == 'imagenet':\r\n",
        "#             self.base.load_param(model_path)\r\n",
        "#             print('Loading pretrained ImageNet model......')\r\n",
        "        self.base=resnet50(True)\r\n",
        "        self.base=nn.Sequential(\r\n",
        "            self.base.conv1,\r\n",
        "            self.base.bn1,\r\n",
        "            self.base.maxpool,\r\n",
        "            self.base.layer1,\r\n",
        "            self.base.layer2,\r\n",
        "            self.base.layer3,\r\n",
        "            self.base.layer4\r\n",
        "        )\r\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\r\n",
        "        # self.gap = nn.AdaptiveMaxPool2d(1)\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.neck = neck\r\n",
        "        self.neck_feat = neck_feat\r\n",
        "\r\n",
        "        if self.neck == 'no':\r\n",
        "            self.classifier = nn.Linear(self.in_planes, self.num_classes)\r\n",
        "            # self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)     # new add by luo\r\n",
        "            # self.classifier.apply(weights_init_classifier)  # new add by luo\r\n",
        "        elif self.neck == 'bnneck':\r\n",
        "            self.bottleneck = nn.BatchNorm1d(self.in_planes)\r\n",
        "            self.bottleneck.bias.requires_grad_(False)  # no shift\r\n",
        "            self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\r\n",
        "\r\n",
        "            self.bottleneck.apply(weights_init_kaiming)\r\n",
        "            self.classifier.apply(weights_init_classifier)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        global_feat = self.gap(self.base(x))  # (b, 2048, 1, 1)\r\n",
        "        global_feat = global_feat.view(global_feat.shape[0], -1)  # flatten to (bs, 2048)\r\n",
        "        out={}\r\n",
        "        if self.neck == 'no':\r\n",
        "            feat = global_feat\r\n",
        "        elif self.neck == 'bnneck':\r\n",
        "            feat = self.bottleneck(global_feat)  # normalize for angular softmax\r\n",
        "\r\n",
        "        if self.training:\r\n",
        "            cls_score = self.classifier(feat)\r\n",
        "            out['logit']=cls_score\r\n",
        "            out['feat']=global_feat\r\n",
        "            return out  # global feature for triplet loss\r\n",
        "        else:\r\n",
        "            if self.neck_feat == 'after':\r\n",
        "                # print(\"Test with feature after BN\")\r\n",
        "                out['feat']=feat\r\n",
        "                return out\r\n",
        "            else:\r\n",
        "                # print(\"Test with feature before BN\")\r\n",
        "                out['feat']=global_feat\r\n",
        "                return out\r\n",
        "\r\n",
        "    def load_param(self, trained_path):\r\n",
        "        param_dict = torch.load(trained_path)\r\n",
        "        for i in param_dict:\r\n",
        "            if 'classifier' in i:\r\n",
        "                continue\r\n",
        "            self.state_dict()[i].copy_(param_dict[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtwWsO03jp5P"
      },
      "source": [
        "#<font color=red>***metric***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PVazPOjtDY"
      },
      "source": [
        "##<font color=blue>***图像***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr1C6ee-kvTT"
      },
      "source": [
        "###<font color=green>***普通map***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4QtUvoajnuC"
      },
      "source": [
        "\r\n",
        "'''reid计算map函数'''\r\n",
        "def MARKET_EVAL_FUNC(dismat,q_pids,g_pids,q_camids,g_camids,max_rank=50):\r\n",
        "    '''\r\n",
        "    :param dismat: 每个query与其对应查询结果的距离\r\n",
        "    :param q_pids: 每个query行人的id\r\n",
        "    :param g_pids: 每个查询结果的行人的id\r\n",
        "    :param q_camids: 摄像头id，对于同一个query，如果查询结果与query都来自一个摄像头，则丢弃\r\n",
        "    :param g_camids: 同上\r\n",
        "    :param max_rank:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    num_q,num_g=dismat.shape\r\n",
        "    if num_g<max_rank:\r\n",
        "        max_rank=num_g\r\n",
        "\r\n",
        "    ##根据相似度进行排序\r\n",
        "    indices=np.argsort(dismat,axis=1)\r\n",
        "\r\n",
        "    ##在排序结果找到同id图片，用于计算cmc\r\n",
        "    matchs=(g_pids[indices]==q_pids[:,np.newaxis]).astype(np.int32)\r\n",
        "\r\n",
        "    all_cmc=[]\r\n",
        "    all_AP=[]\r\n",
        "    num_valid_q=0\r\n",
        "\r\n",
        "    for q_idx in range(num_q):\r\n",
        "        q_pid=q_pids[q_idx]\r\n",
        "        q_camid=q_camids[q_idx]\r\n",
        "\r\n",
        "        '''当前query对应查询结果的排序'''\r\n",
        "        order=indices[q_idx]\r\n",
        "\r\n",
        "        '''若查询结果与query同摄像机且同id，删除'''\r\n",
        "        remove=(g_pids[order]==q_pid)&(g_camids[order]==q_camid)\r\n",
        "        keep=np.invert(remove)\r\n",
        "\r\n",
        "        '''计算CMC'''\r\n",
        "\r\n",
        "        '''首先先筛选'''\r\n",
        "        orig_cmc=matchs[q_idx][keep]\r\n",
        "\r\n",
        "        '''全是false'''\r\n",
        "        if not np.any(orig_cmc):\r\n",
        "            continue\r\n",
        "\r\n",
        "        '''累加和，因为orig_cmc里面是true，false，true为1，得到cmc列表'''\r\n",
        "        cmc=orig_cmc.cumsum()\r\n",
        "        cmc[cmc>1]=1\r\n",
        "        all_cmc.append(cmc[:max_rank])\r\n",
        "        num_valid_q+=1\r\n",
        "\r\n",
        "        '''计算map'''\r\n",
        "\r\n",
        "        '''预测正确的数量'''\r\n",
        "        num_rel=orig_cmc.sum()\r\n",
        "\r\n",
        "        '''累加和，用于计算'''\r\n",
        "        tmp_cmc=orig_cmc.cumsum()\r\n",
        "\r\n",
        "        '''计算准确率，前k个查询结果的准确率'''\r\n",
        "        tmp_cmc=[x/(i+1) for i,x in enumerate(tmp_cmc)]\r\n",
        "\r\n",
        "        '''计算召回率，召回率每次变化都是当查询结果id等于query时，故乘orig_cmc'''\r\n",
        "        '''最终的结果类似 0,0,1,0,0,2,0,0,3这种，每个有值的都是查询正确的'''\r\n",
        "        '''然后后面除以查询结果中同id数量，也就是上面num_rel就是找召回率'''\r\n",
        "        '''但是这里其实已经提前乘上准确率了，最终结果类似于 0,0,1/3,0,0,2/6,0,0,3/9'''\r\n",
        "        tmp_cmc=np.asarray(tmp_cmc)*orig_cmc\r\n",
        "\r\n",
        "        '''以上为例，除以同id数量后，假设为五个'''\r\n",
        "        '''0，0，1/3 /5，0，0，2/6 /5，0，0，3/9 /5'''\r\n",
        "        '''等价于 0,0, 1/3(准确率)*1/5(召回率) ....'''\r\n",
        "        AP = tmp_cmc.sum() / num_rel\r\n",
        "        all_AP.append(AP)\r\n",
        "\r\n",
        "    all_cmc = np.asarray(all_cmc).astype(np.float32)\r\n",
        "    all_cmc = all_cmc.sum(0) / num_valid_q\r\n",
        "    mAP = np.mean(all_AP)\r\n",
        "\r\n",
        "    return all_cmc, mAP\r\n",
        "\r\n",
        "\r\n",
        "class MARKET_MAP():\r\n",
        "    def __init__(self,num_query,max_rank=50,feat_norm=True):\r\n",
        "        '''\r\n",
        "\r\n",
        "        :param num_query: 查询数量\r\n",
        "        :param max_rank: 每个query限定的最大查询结果数量\r\n",
        "        :param feat_norm:是否对特征进行l2norm\r\n",
        "        '''\r\n",
        "        self.num_query=num_query\r\n",
        "        self.max_rank=max_rank\r\n",
        "        self.feat_norm=feat_norm\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        '''\r\n",
        "        feats:模型返回的特征\r\n",
        "        pids: 图片中行人的id\r\n",
        "        camids:图片摄像头的id\r\n",
        "        :return:\r\n",
        "        '''\r\n",
        "        self.feats=[]\r\n",
        "        self.pids=[]\r\n",
        "        self.camids=[]\r\n",
        "\r\n",
        "    def update(self,output):\r\n",
        "        feat,pid,camid=output\r\n",
        "    \r\n",
        "        self.feats.append(feat)\r\n",
        "        self.pids.append(pid)\r\n",
        "        self.camids.append(camid)\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        feats=torch.stack(self.feats,dim=0)\r\n",
        "       \r\n",
        "        if self.feat_norm:\r\n",
        "            feats=torch.nn.functional.normalize(feats,p=2)\r\n",
        "\r\n",
        "        '''用于查询的图片的特征及其id和摄像头id'''\r\n",
        "        qf=feats[:self.num_query]\r\n",
        "        q_pids=np.asarray(self.pids[:self.num_query])\r\n",
        "        q_camids = np.asarray(self.camids[:self.num_query])\r\n",
        "\r\n",
        "        '''每个query查询结果的特征及其id和摄像头id'''\r\n",
        "        gf = feats[self.num_query:]\r\n",
        "        g_pids = np.asarray(self.pids[self.num_query:])\r\n",
        "        g_camids = np.asarray(self.camids[self.num_query:])\r\n",
        "\r\n",
        "        m,n=qf.shape[0],gf.shape[0]\r\n",
        "     \r\n",
        "        '''计算qf每个query和对应查询结果的距离'''\r\n",
        "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "       \r\n",
        "        distmat.addmm_(1, -2, qf, gf.t())\r\n",
        "        distmat = distmat.cpu().numpy()\r\n",
        "      \r\n",
        "        cmc,mAP=MARKET_EVAL_FUNC(distmat,q_pids,g_pids,q_camids,g_camids)\r\n",
        "        return cmc,mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GcAv-6SlFfh"
      },
      "source": [
        "###<font color=green>***rerank map***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLgr8-fllG_a"
      },
      "source": [
        "def RE_RANKING(porbFea,galFea,k1,k2,lambda_value,local_distmat=None,only_local=False):\r\n",
        "    '''\r\n",
        "    :param porbFea:query特征\r\n",
        "    :param galFea: 查询结果特征\r\n",
        "    :param k1: 一开始k近邻的k\r\n",
        "    :param k2:\r\n",
        "    :param lambda_value:\r\n",
        "    :param locl_distmat:\r\n",
        "    :param only_loca:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    query_num=porbFea.shape[0]\r\n",
        "    all_num=query_num+galFea.shape[0]\r\n",
        "    if only_local:\r\n",
        "        original_dist=local_distmat\r\n",
        "    else:\r\n",
        "        '''把query加进去，然后算每个图片之间的距离，用于后面reranking'''\r\n",
        "        feat=torch.cat([porbFea,galFea])\r\n",
        "        distmat = torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num) + \\\r\n",
        "                  torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()\r\n",
        "        distmat.addmm_(1, -2, feat, feat.t())\r\n",
        "        original_dist = distmat.cpu().numpy()\r\n",
        "        del feat\r\n",
        "        if not local_distmat is None:\r\n",
        "            original_dist = original_dist + local_distmat\r\n",
        "    gallery_num=original_dist.shape[0]\r\n",
        "\r\n",
        "    '''进行归一化，然后进行转置，得到归一化的距离矩阵'''\r\n",
        "    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))\r\n",
        "\r\n",
        "    V=np.zeros_like(original_dist).astype(np.float16)\r\n",
        "\r\n",
        "    '''初始rank列表就是直接根据距离最小值'''\r\n",
        "    initial_rank=np.argsort(original_dist).astype(np.int32)\r\n",
        "\r\n",
        "    '''进行重排序'''\r\n",
        "    '''最核心的思想就是对于query，前k个距离最近的查询结果，每一个查询结果在所有图片中的k近邻\r\n",
        "    应该包含query，根据这个进行重排，使得rerank后尽量满足这个条件'''\r\n",
        "    for i in range(all_num):\r\n",
        "        '''选择当前图片的k近邻'''\r\n",
        "        forward_k_neigh_index=initial_rank[i,:k1+1]\r\n",
        "\r\n",
        "        '''选择k近邻的k近邻'''\r\n",
        "        backward_k_neigh_index=initial_rank[forward_k_neigh_index,:k1+1]\r\n",
        "\r\n",
        "        '''满足上面的条件，k近邻的k近邻包含当前图片'''\r\n",
        "        fi=np.where(backward_k_neigh_index==i)[0]\r\n",
        "\r\n",
        "        '''那么满足上面条件的k近邻作为rerank的候选集'''\r\n",
        "        k_reciprocal_index=forward_k_neigh_index[fi]\r\n",
        "        k_reciprocal_expansion_index = k_reciprocal_index\r\n",
        "\r\n",
        "        '''如果满足上面的条件，通常就是正样本无误了，这也就是rerank的目的'''\r\n",
        "        '''但是问题在于如果因为一些干扰，导致正样本在k1近邻之外，那rerank就没用'''\r\n",
        "        '''所以提出一个方法进行候选集的扩充，希望能够保证正样本进入候选集'''\r\n",
        "\r\n",
        "        '''基本的思想就是对于当前图片，我们找到了符合条件的原候选集'''\r\n",
        "        '''那么对于候选集每一个元素，找到以一半k1为长度的满足上面条件的候选集'''\r\n",
        "        '''如果这个候选集与原候选集的交集超过这个候选集的2/3长度，那么把这个候选集加入原候选集'''\r\n",
        "        for j in range(len(k_reciprocal_index)):\r\n",
        "            '''原候选集每个元素'''\r\n",
        "            candidate=k_reciprocal_index[j]\r\n",
        "            '''找到当前元素满足条件的候选集'''\r\n",
        "            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]\r\n",
        "            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index,\r\n",
        "                                               :int(np.around(k1 / 2)) + 1]\r\n",
        "            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]\r\n",
        "            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]\r\n",
        "\r\n",
        "            '''如果交集长度超过这个候选集的2/3长度'''\r\n",
        "            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))>(2/3)*len(candidate_k_reciprocal_index):\r\n",
        "                k_reciprocal_expansion_index=np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\r\n",
        "        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\r\n",
        "\r\n",
        "        '''获取当前图片及其候选集的距离，进行exp'''\r\n",
        "        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])\r\n",
        "\r\n",
        "        '''找到每个图片与其候选集的一个权值表示，用于后面计算jaccard距离'''\r\n",
        "        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)\r\n",
        "    original_dist=original_dist[:query_num,]\r\n",
        "\r\n",
        "    '''这部分处理时针对线上reid系统，因为gallery通常是一直更新的，由于计算V的过程比较麻烦'''\r\n",
        "    '''我们只能隔一段时间计算，那么对于新加入的图片，其在V中没有值，所以采用这个近似方法来计算'''\r\n",
        "    if k2!=1:\r\n",
        "        V_qe=np.zeros_like(V,dtype=np.float16)\r\n",
        "        for i in range(all_num):\r\n",
        "            '''采用查询样本的特征均值代替上面的reciprocal编码，只是近似处理'''\r\n",
        "            V_qe[i,:]=np.mean(V[initial_rank[i,:k2],:],axis=0)\r\n",
        "        V=V_qe\r\n",
        "        del V_qe\r\n",
        "    del initial_rank\r\n",
        "    invIndex=[]\r\n",
        "    for i in range(gallery_num):\r\n",
        "        '''找到候选集包含当前图片的所有query'''\r\n",
        "        invIndex.append(np.where(V[:,i]!=0)[0])\r\n",
        "\r\n",
        "    '''计算jaccard距离'''\r\n",
        "    jaccard_dist=np.zeros_like(original_dist,dtype=np.float16)\r\n",
        "\r\n",
        "    '''jaccard距离的计算，对于query q和其中一个查询样本gi的距离，我们用1-其候选集的交并比来代表，如果交并比越大，代表两者'''\r\n",
        "    '''候选集很接近，说明两者很接近，则其距离就很小'''\r\n",
        "\r\n",
        "    '''为了方便表示交集和并集，我们建立Vq，其中V(q,gi)代表gi是否在q候选集中，如果在为1，否则为0，那么这个Vq就是针对所有图片，类似候选集的one-hot'''\r\n",
        "    '''Vgi同理，但是one-hot是hard编码，为了soft编码，我们使用上面np.exp(-距离）的计算'''\r\n",
        "\r\n",
        "    '''然后交集怎么表示呢？ sum(min(vq,vgi))，就是每个位置取Vq和Vgi的最小值,然后求和，为什么呢？我们以one-hot去想，交集意味着两者一样，那么对于相同位置，只有Vq和Vgi在这个位置\r\n",
        "    都为1，那么这个位置才为1，否则为0，所以求和的时候就是求1的个数，相当于求对应位置是相同的个数，这就是交集的定义'''\r\n",
        "\r\n",
        "    '''并集是sum(max(Vq,Vgi)),意义同上。'''\r\n",
        "\r\n",
        "    for i in range(query_num):\r\n",
        "        temp_min=np.zeros(shape=[1,gallery_num],dtype=np.float16)\r\n",
        "        '''找到当前图片的候选集'''\r\n",
        "        indNonZero=np.where(V[i,:]!=0)[0]\r\n",
        "\r\n",
        "        '''候选集的候选集'''\r\n",
        "        indImages=[invIndex[ind] for ind in indNonZero]\r\n",
        "        '''交集的计算'''\r\n",
        "        '''不断求和，看公式'''\r\n",
        "        for j in range(len(indNonZero)):\r\n",
        "               temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]],\r\n",
        "                                                                                       V[indImages[j], indNonZero[j]])\r\n",
        "        '''2-temp_min是并集的快速计算'''\r\n",
        "        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)\r\n",
        "\r\n",
        "\r\n",
        "    '''作为最终的距离矩阵'''\r\n",
        "    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value\r\n",
        "    del original_dist\r\n",
        "    del V\r\n",
        "    del jaccard_dist\r\n",
        "    final_dist = final_dist[:query_num, query_num:]\r\n",
        "    return final_dist\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "'''使用reranking技术后的准确度'''\r\n",
        "class MARKET_mAP_reranking():\r\n",
        "    def __init__(self, num_query, max_rank=50, feat_norm=True):\r\n",
        "        self.num_query = num_query\r\n",
        "        self.max_rank = max_rank\r\n",
        "        self.feat_norm = feat_norm\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.feats = []\r\n",
        "        self.pids = []\r\n",
        "        self.camids = []\r\n",
        "\r\n",
        "    def update(self, output):\r\n",
        "        feat, pid, camid = output\r\n",
        "        self.feats.append(feat)\r\n",
        "        self.pids.append(pid)\r\n",
        "        self.camids.append(camid)\r\n",
        "\r\n",
        "    def compute(self):\r\n",
        "        feats = torch.stack(self.feats, dim=0)\r\n",
        "        if self.feat_norm :\r\n",
        "           \r\n",
        "            feats = torch.nn.functional.normalize(feats, p=2)\r\n",
        "\r\n",
        "        # query\r\n",
        "        qf = feats[:self.num_query]\r\n",
        "        q_pids = np.asarray(self.pids[:self.num_query])\r\n",
        "        q_camids = np.asarray(self.camids[:self.num_query])\r\n",
        "        # gallery\r\n",
        "        gf = feats[self.num_query:]\r\n",
        "        g_pids = np.asarray(self.pids[self.num_query:])\r\n",
        "        g_camids = np.asarray(self.camids[self.num_query:])\r\n",
        "        # m, n = qf.shape[0], gf.shape[0]\r\n",
        "        # distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\r\n",
        "        #           torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\r\n",
        "        # distmat.addmm_(1, -2, qf, gf.t())\r\n",
        "        # distmat = distmat.cpu().numpy()\r\n",
        "        print(\"Enter reranking\")\r\n",
        "        distmat = RE_RANKING(qf, gf, k1=20, k2=6, lambda_value=0.3)\r\n",
        "        cmc, mAP = MARKET_EVAL_FUNC(distmat, q_pids, g_pids, q_camids, g_camids)\r\n",
        "\r\n",
        "        return cmc, mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwMCZH4-lXBK"
      },
      "source": [
        "# <font color=red>***Loss***\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1OHkqhMlY8T"
      },
      "source": [
        "##<font color=blue>***分类损失***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeL1k-gLlYKF"
      },
      "source": [
        "class CrossEntropyLabelSmoothLoss(nn.Module):\r\n",
        "\t\"\"\"Cross entropy loss with label smoothing regularizer.\r\n",
        "\tReference:\r\n",
        "\tSzegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\r\n",
        "\tEquation: y = (1 - epsilon) * y + epsilon / K.\r\n",
        "\tArgs:\r\n",
        "\t\tnum_classes (int): number of classes.\r\n",
        "\t\tepsilon (float): weight.\r\n",
        "\t\"\"\"\r\n",
        "\tdef __init__(self, num_classes, epsilon=0.1, use_gpu=True):\r\n",
        "\t\tsuper(CrossEntropyLabelSmoothLoss, self).__init__()\r\n",
        "\t\tself.num_classes = num_classes\r\n",
        "\t\tself.epsilon = epsilon\r\n",
        "\t\tself.use_gpu = use_gpu\r\n",
        "\t\tself.logsoftmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "\tdef forward(self, inputs, targets):\r\n",
        "\t\t\"\"\"\r\n",
        "\t\tArgs:\r\n",
        "\t\t\tinputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\r\n",
        "\t\t\ttargets: ground truth labels with shape (num_classes)\r\n",
        "\t\t\"\"\"\r\n",
        " \r\n",
        "\t\tlog_probs = self.logsoftmax(inputs['logit'])\r\n",
        "\t\ttargets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).cpu(), 1)\r\n",
        "\t\tif self.use_gpu: targets = targets.cuda()\r\n",
        "\t\ttargets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\r\n",
        "\t\tloss = (- targets * log_probs).mean(0).sum()\r\n",
        "\t\treturn loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbvoFc_kld7d"
      },
      "source": [
        "## <font color=blue>***度量学习***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grBozebilpOY"
      },
      "source": [
        "###<font color=green>***triplet***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YutBFMCOlgN3"
      },
      "source": [
        "   \r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def normalize(x, axis=-1):\r\n",
        "\t\"\"\"Normalizing to unit length along the specified dimension.\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable\r\n",
        "\tReturns:\r\n",
        "\t  x: pytorch Variable, same shape as input\r\n",
        "\t\"\"\"\r\n",
        "\tx = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12)\r\n",
        "\treturn x\r\n",
        "\r\n",
        "def euclidean_dist(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\tReturns:\r\n",
        "\t  dist: pytorch Variable, with shape [m, n]\r\n",
        "\t\"\"\"\r\n",
        "\tm, n = x.size(0), y.size(0)\r\n",
        "\txx = torch.pow(x.float(), 2).sum(1, keepdim=True).expand(m, n)\r\n",
        "\tyy = torch.pow(y.float(), 2).sum(1, keepdim=True).expand(n, m).t()\r\n",
        "\tdist = xx + yy\r\n",
        "\tdist.addmm_(1, -2, x.float(), y.t().float())\r\n",
        "\tdist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\r\n",
        "\treturn dist\r\n",
        "\r\n",
        "def cosine_dist(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\t\"\"\"\r\n",
        "\tx_normed = F.normalize(x, p=2, dim=1)\r\n",
        "\ty_normed = F.normalize(y, p=2, dim=1)\r\n",
        "\treturn 1 - torch.mm(x_normed, y_normed.t())\r\n",
        "\r\n",
        "def cosine_similarity(x, y):\r\n",
        "\t\"\"\"\r\n",
        "\tArgs:\r\n",
        "\t  x: pytorch Variable, with shape [m, d]\r\n",
        "\t  y: pytorch Variable, with shape [n, d]\r\n",
        "\t\"\"\"\r\n",
        "\tx_normed = F.normalize(x, p=2, dim=1)\r\n",
        "\ty_normed = F.normalize(y, p=2, dim=1)\r\n",
        "\treturn torch.mm(x_normed, y_normed.t())\r\n",
        "\r\n",
        "\r\n",
        "def hard_example_mining(dist_mat, labels, return_inds=False):\r\n",
        "\t\"\"\"For each anchor, find the hardest positive and negative sample.\r\n",
        "\tArgs:\r\n",
        "\t  dist_mat: pytorch Variable, pair wise distance between samples, shape [N, N]\r\n",
        "\t  labels: pytorch LongTensor, with shape [N]\r\n",
        "\t  return_inds: whether to return the indices. Save time if `False`(?)\r\n",
        "\tReturns:\r\n",
        "\t  dist_ap: pytorch Variable, distance(anchor, positive); shape [N]\r\n",
        "\t  dist_an: pytorch Variable, distance(anchor, negative); shape [N]\r\n",
        "\t  p_inds: pytorch LongTensor, with shape [N];\r\n",
        "\t\tindices of selected hard positive samples; 0 <= p_inds[i] <= N - 1\r\n",
        "\t  n_inds: pytorch LongTensor, with shape [N];\r\n",
        "\t\tindices of selected hard negative samples; 0 <= n_inds[i] <= N - 1\r\n",
        "\tNOTE: Only consider the case in which all labels have same num of samples,\r\n",
        "\t  thus we can cope with all anchors in parallel.\r\n",
        "\t\"\"\"\r\n",
        "\tassert len(dist_mat.size()) == 2\r\n",
        "\tassert dist_mat.size(0) == dist_mat.size(1)\r\n",
        "\tN = dist_mat.size(0)\r\n",
        "\r\n",
        "\tis_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\r\n",
        "\tis_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\r\n",
        "\r\n",
        "\tdist_ap, relative_p_inds = torch.max(\r\n",
        "\t\tdist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\r\n",
        "\tdist_an, relative_n_inds = torch.min(\r\n",
        "\t\tdist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\r\n",
        "\t\r\n",
        "\tdist_ap = dist_ap.squeeze(1)\r\n",
        "\tdist_an = dist_an.squeeze(1)\r\n",
        "\r\n",
        "\tif return_inds:\r\n",
        "\t\tind = (labels.new().resize_as_(labels)\r\n",
        "\t\t\t   .copy_(torch.arange(0, N).long())\r\n",
        "\t\t\t   .unsqueeze(0).expand(N, N))\r\n",
        "\t\tp_inds = torch.gather(\r\n",
        "\t\t\tind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\r\n",
        "\t\tn_inds = torch.gather(\r\n",
        "\t\t\tind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\r\n",
        "\t\tp_inds = p_inds.squeeze(1)\r\n",
        "\t\tn_inds = n_inds.squeeze(1)\r\n",
        "\t\treturn dist_ap, dist_an, p_inds, n_inds\r\n",
        "\r\n",
        "\treturn dist_ap, dist_an\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class TripletHardLoss(object):\r\n",
        "\t\"\"\"Modified from Tong Xiao's open-reid (https://github.com/Cysu/open-reid).\r\n",
        "\tRelated Triplet Loss theory can be found in paper 'In Defense of the Triplet\r\n",
        "\tLoss for Person Re-Identification'.\"\"\"\r\n",
        "\tdef __init__(self, margin=None, metric=\"euclidean\"):\r\n",
        "\t\tself.margin = margin\r\n",
        "\t\tself.metric = metric\r\n",
        "\t\tif margin is not None:\r\n",
        "\t\t\tself.ranking_loss = nn.MarginRankingLoss(margin=margin)\r\n",
        "\t\telse:\r\n",
        "\t\t\tself.ranking_loss = nn.SoftMarginLoss()\r\n",
        "\r\n",
        "\tdef __call__(self, outs, labels, normalize_feature=True):\r\n",
        "   \r\n",
        "\t\tif normalize_feature:\r\n",
        "\t\t\tglobal_feat = normalize(outs['feat'], axis=-1)\r\n",
        "     \r\n",
        "\t\tif self.metric == \"euclidean\":\r\n",
        "\t\t\tdist_mat = euclidean_dist(outs['feat'], global_feat)\r\n",
        "\t\telif self.metric == \"cosine\":\r\n",
        "\t\t\tdist_mat = cosine_dist(outs['feat'], global_feat)\r\n",
        "\t\telse:\r\n",
        "\t\t\traise NameError\r\n",
        "\r\n",
        "\t\tdist_ap, dist_an = hard_example_mining(\r\n",
        "\t\t\tdist_mat, labels)\r\n",
        "\t\ty = dist_an.new().resize_as_(dist_an).fill_(1)\r\n",
        "\t\t\r\n",
        "\t\tif self.margin is not None:\r\n",
        "\t\t\tloss = self.ranking_loss(dist_an, dist_ap, y)\r\n",
        "\t\telse:\r\n",
        "\t\t\tloss = self.ranking_loss(dist_an - dist_ap, y)\r\n",
        "\t\tprec = (dist_an.data > dist_ap.data).sum() * 1. / y.size(0)\r\n",
        "\t\treturn loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DoBceFMJbxp"
      },
      "source": [
        "### <font color=green>***center loss***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8nKAjlSoOk8"
      },
      "source": [
        "\r\n",
        "class CenterLoss(nn.Module):\r\n",
        "    \"\"\"Center loss.\r\n",
        "    Reference:\r\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\r\n",
        "    Args:\r\n",
        "        num_classes (int): number of classes.\r\n",
        "        feat_dim (int): feature dimension.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, num_classes=751, feat_dim=2048, use_gpu=True):\r\n",
        "        super(CenterLoss, self).__init__()\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.feat_dim = feat_dim\r\n",
        "        self.use_gpu = use_gpu\r\n",
        "\r\n",
        "        if self.use_gpu:\r\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\r\n",
        "        else:\r\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\r\n",
        "\r\n",
        "    def forward(self, x, labels):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\r\n",
        "            labels: ground truth labels with shape (num_classes).\r\n",
        "        \"\"\"\r\n",
        "        x=x['feat']\r\n",
        "        assert x.size(0) == labels.size(0), \"features.size(0) is not equal to labels.size(0)\"\r\n",
        "\r\n",
        "        batch_size = x.size(0)\r\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\r\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\r\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\r\n",
        "\r\n",
        "        classes = torch.arange(self.num_classes).long()\r\n",
        "        if self.use_gpu: classes = classes.cuda()\r\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\r\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\r\n",
        "\r\n",
        "        dist = distmat * mask.float()\r\n",
        "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\r\n",
        "        #dist = []\r\n",
        "        #for i in range(batch_size):\r\n",
        "        #    value = distmat[i][mask[i]]\r\n",
        "        #    value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\r\n",
        "        #    dist.append(value)\r\n",
        "        #dist = torch.cat(dist)\r\n",
        "        #loss = dist.mean()\r\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2TJdWOoucU"
      },
      "source": [
        "##训练loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwD9tdg_o5Cl"
      },
      "source": [
        "# <font color=red>***lr_scheduler***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2XalTTRvhz2"
      },
      "source": [
        "## <font color=blue>***warm up***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1YdaRgToxrD"
      },
      "source": [
        "from bisect import bisect_right\r\n",
        "import torch\r\n",
        "\r\n",
        "class WarmupMultiStepLR:\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        optimizer,\r\n",
        "        milestones,\r\n",
        "        base_lr,\r\n",
        "        gamma=0.1,\r\n",
        "        warmup_factor=1.0 / 3,\r\n",
        "        warmup_iters=500,\r\n",
        "        warmup_method=\"linear\",\r\n",
        "        last_epoch=-1,\r\n",
        "    ):\r\n",
        "        if not list(milestones) == sorted(milestones):\r\n",
        "            raise ValueError(\r\n",
        "                \"Milestones should be a list of\" \" increasing integers. Got {}\",\r\n",
        "                milestones,\r\n",
        "            )\r\n",
        "\r\n",
        "        if warmup_method not in (\"constant\", \"linear\"):\r\n",
        "            raise ValueError(\r\n",
        "                \"Only 'constant' or 'linear' warmup_method accepted\"\r\n",
        "                \"got {}\".format(warmup_method)\r\n",
        "            )\r\n",
        "        self.base_lr=base_lr\r\n",
        "        self.milestones = milestones\r\n",
        "        self.gamma = gamma\r\n",
        "        self.warmup_factor = warmup_factor\r\n",
        "        self.warmup_iters = warmup_iters\r\n",
        "        self.warmup_method = warmup_method\r\n",
        "       \r\n",
        "\r\n",
        "\r\n",
        "    def get_lr(self):\r\n",
        "        warmup_factor = 1\r\n",
        "        if self.last_epoch < self.warmup_iters:\r\n",
        "            if self.warmup_method == \"constant\":\r\n",
        "                warmup_factor = self.warmup_factor\r\n",
        "            elif self.warmup_method == \"linear\":\r\n",
        "                alpha = self.last_epoch / self.warmup_iters\r\n",
        "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\r\n",
        "\r\n",
        "        return self.base_lr* warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\r\n",
        "            \r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VITUywn2w5bK"
      },
      "source": [
        "#<font color=red>***自定义optimizer***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiReKXojxGop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pep0zTlKO7S"
      },
      "source": [
        "#<font color=red>***自定义eva***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VZu_FcobkO4"
      },
      "source": [
        "##<font color=blue>***普通***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6igUDwI5azrU"
      },
      "source": [
        "def do_eva1(cfg,model,val_loader,num_query):\r\n",
        "  model.eval()\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  if cfg['rerank']:\r\n",
        "    metric=MARKET_mAP_reranking(num_query)\r\n",
        "  else:\r\n",
        "    metric=MARKET_MAP(num_query)\r\n",
        "  \r\n",
        "  with torch.no_grad():\r\n",
        "    for imgs,pids,cams in val_loader:\r\n",
        "      imgs=imgs.to(device)\r\n",
        "      outs=model(imgs)\r\n",
        "      for out in zip(outs['feat'],pids,cams):\r\n",
        "        \r\n",
        "        metric.update(out)\r\n",
        "  \r\n",
        "  cmc,map=metric.compute()\r\n",
        "  return cmc,map\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OsA3g4ntlgV"
      },
      "source": [
        "#<font color=red>***训练辅助函数***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryoETDPctsIv"
      },
      "source": [
        "##<font color=blue>***checkpoint***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZF5we9Ct47q"
      },
      "source": [
        "class Checkpoint:\r\n",
        "  def __init__(self,ckpt):\r\n",
        "    self.ckpt=ckpt\r\n",
        "    self.init=-1000\r\n",
        "  \r\n",
        "  def __call__(self,metric,model,optimizer):\r\n",
        "    if metric>self.init:\r\n",
        "      self.init=metric\r\n",
        "      torch.save({'model':model.state_dict(),'optimizer':optimizer.state_dict()},self.ckpt)\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z61u-GZxI86"
      },
      "source": [
        "#<font color=red>***自定义训练过程***</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKuzTaBDxUjt"
      },
      "source": [
        "##<font color=blue>strong_baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgifGg5XxPii"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import logging\r\n",
        "def do_train1(cfg,model,train_loader,val_loader,optimizer,scheduler,criterion,num_query,logger,checkpoint):\r\n",
        "  device=torch.device('cuda:0')\r\n",
        "  model=model.to(device)\r\n",
        "  epochs=cfg['epochs']\r\n",
        "  if cfg['amp']:\r\n",
        "    scaler=GradScaler()\r\n",
        "  with tqdm(total=epochs) as pbar:\r\n",
        "    for epoch in range(epochs):\r\n",
        "      model.train()\r\n",
        "      scheduler.last_epoch=epoch+1\r\n",
        "      lr=scheduler.get_lr()\r\n",
        "      for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr\r\n",
        "\r\n",
        "      for images,pids in train_loader:\r\n",
        "        loss=0\r\n",
        "        optimizer.zero_grad()\r\n",
        "        images=images.to(device)\r\n",
        "        \r\n",
        "        if cfg['amp']:\r\n",
        "          with autocast():\r\n",
        "            outs = model(images)\r\n",
        "            pids=pids.to(device)\r\n",
        "            for cri,weight in criterion:\r\n",
        "              loss=loss+weight*cri(outs,pids)\r\n",
        "          scaler.scale(loss).backward()\r\n",
        "          scaler.step(optimizer)\r\n",
        "          scaler.update()  \r\n",
        "        else:\r\n",
        "          outs = model(images)\r\n",
        "          pids=pids.to(device)\r\n",
        "          for cri,weight in criterion:\r\n",
        "            loss=loss+weight*cri(outs,pids)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "      \r\n",
        "      if epoch%5==0:\r\n",
        "        cmc,map=do_eva1(cfg,model,val_loader,num_query)\r\n",
        "        logger.info('epoch:{}---cmc:{}---map:{}----'.format(epoch,cmc,map))\r\n",
        "        checkpoint(map,model,optimizer)\r\n",
        "      pbar.update(1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX-nyeucxwns"
      },
      "source": [
        "# <font color=red>***训练***</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6TwHwQSx1rA",
        "outputId": "2a4243be-9c96-4a01-da82-2e6555bed225"
      },
      "source": [
        "dataset=Market1501()\r\n",
        "train_loader,val_loader,num_query,num_class=make_data_loader(dataset,cfg)\r\n",
        "\r\n",
        "\r\n",
        "model=Baseline(num_classes)\r\n",
        "\r\n",
        "\r\n",
        "criterion_cls=CrossEntropyLabelSmoothLoss(num_classes)\r\n",
        "criterion_tri=TripletHardLoss(cfg['margin'])\r\n",
        "criterion=[(criterion_cls,1.0),(criterion_tri,1.0)]\r\n",
        "\r\n",
        "\r\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\r\n",
        "\r\n",
        "scheduler = WarmupMultiStepLR(optimizer,cfg['steps'],cfg['lr'], cfg['gamma'],\r\n",
        "               cfg['warmup_factors'],cfg['warmup_iters'], \r\n",
        "               'linear', 0)\r\n",
        "\r\n",
        "ckpt=cfg['ckpt']\r\n",
        "logpt=cfg['logpt']\r\n",
        "logger=logging.getLogger()\r\n",
        "fh = logging.FileHandler(\"spam.log\")\r\n",
        "fh.setLevel(logging.DEBUG)\r\n",
        "logger.addHandler(fh)\r\n",
        "checkpoint=Checkpoint(ckpt)\r\n",
        "do_train1(cfg,model,train_loader,val_loader,optimizer,scheduler,criterion,num_query,\r\n",
        "          logger,checkpoint)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> Market1501 loaded\n",
            "Dataset statistics:\n",
            "  ----------------------------------------\n",
            "  subset   | # ids | # images | # cameras\n",
            "  ----------------------------------------\n",
            "  train    |   751 |    12936 |         6\n",
            "  query    |   750 |     3368 |         6\n",
            "  gallery  |   751 |    15913 |         6\n",
            "  ----------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 12/70 [15:41<1:16:40, 79.31s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vNQkuQJNX0J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}